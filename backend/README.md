# Multi-modal Search Backend

This directory contains a Flask application that powers the MMâ€‘Search project. It exposes APIs for uploading files, background processing, semantic search and summarisation. Vector data is stored in Qdrant and language model calls are handled locally via Ollama.

## Setup

1. Create a Python virtual environment and install dependencies:
   ```bash
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```
2. Copy `.env.example` to `.env` and adjust settings.
3. Run the application:
   ```bash
   python run.py
   ```
   This starts the Flask development server on port `5050` and creates database tables automatically.

Logs are written to the file specified by the `LOG_FILE` environment variable (default `app.log`).

## Sample cURL Requests

An example script with `curl` commands for every API endpoint is provided in `api_examples.sh`. Set `BASE_URL` to the address of the running Flask server and execute the script to test the API:

```bash
bash api_examples.sh
```

Edit the file paths or IDs in the script as needed for your environment.

## Summarisation

The search API returns a `summary` field generated by the `llama3.2:3b` model. The summary provides a short inference over the search results relevant to the given query.

## Gradio Demo

A small Gradio interface is available for manual testing. It can upload a file to `/api/upload/file` and send questions to `/api/search`.

Run it with:

```bash
python gradio_demo.py
```

The demo launches a local Gradio web UI that interacts with the running backend.
